{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curve similarity to extend dataset (failed)\n",
    "\n",
    "Since our drive dataset is too small in order to build a classifier on it, we \n",
    "need to extend the set. \n",
    "\n",
    "#### How are we gonna do it?\n",
    "\n",
    "We collected a lot of timerelated data, which we will not have in real world application of this classifier.\n",
    "Based on that timerelated data, we want to predict whether a article is evergreen or not.\n",
    "This may be evaluated by the views in the days after publication. \n",
    "\n",
    "Steps:\n",
    "- Use the statistic normalization function\n",
    "- Filter out outliners (flatten peaks)\n",
    "- Just take a range of days to observe\n",
    "- Convert views_for_certain day into vectors (each element represents one day)\n",
    "- Convert the curves into vectors\n",
    "- Use cosine similarity to observe which article would be likely a evergreen\n",
    "- look at which threshhold of similarity the highest accuracy can be noticed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# load labels \n",
    "labels = pd.read_json('project_3_labels.json')\n",
    "# load dataframe with all including content informations\n",
    "df_content = pd.read_csv('EDA_Evergreen.csv')\n",
    "#loading timeelements\n",
    "df_pageviews = pd.read_csv('evergreen_pagereads')\n",
    "# load wordbags\n",
    "data_dtm_body = pd.read_csv('data_dtm_title.csv')\n",
    "data_dtm_title = pd.read_csv('data_dtm_body.csv')\n",
    "\n",
    "del df_content['Unnamed: 0']\n",
    "del df_content['id']\n",
    "del df_content['text']\n",
    "del df_content['meta.weeks_with_more_than_50_clicks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting only relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one df based on content and pageviews\n",
    "df = df_pageviews.join(df_content.set_index('article_drive_id')[['meta.publisher','label_id','label_text',\n",
    "                                                                 'title','text_content','topic',\n",
    "                                                                 'locality','newstype','genre']], on='article_drive_id')\n",
    "\n",
    "# drop duplicates generated by join\n",
    "df.drop_duplicates(subset=['article_drive_id', 'days_past', 'views_for_certain_day'],inplace= True)\n",
    "\n",
    "# deleting saisonal articles\n",
    "df = df.loc[df['label_id']!=173]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all drive_id's for iterating later\n",
    "unique_drive_id = set(df['article_drive_id'])\n",
    "\n",
    "# extracting evergreentypes into own df's\n",
    "for i in range(len(unique_drive_id)):\n",
    "    df_non_ev = df.loc[df['label_id'] == 175]\n",
    "    df_event_ev = df.loc[df['label_id'] == 174]\n",
    "    df_zeitlos_ev = df.loc[df['label_id'] == 172]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Statistic Normalization Function\n",
    "\n",
    "Since some publishers and articles are more famous than others and also that evergreens have a high decrease in views in the first days, we are normalizing based on the given days.\n",
    "\n",
    "Thus, we can highlight the behavior in the first days and differ between evergreens and non evergreens. Note that normalization function such as Softmax or Symmetric normalization ruin structures of the evergreen article due to the high amount of nonevergreen articles\n",
    "\n",
    "We take a normalization function. This function takes the first x days to calculate its average and than normalize the past days  based on that.\n",
    "\n",
    "\n",
    "- #### days and average_of_days can be used as hyper parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generiert die Vektorlänge bzw. die Anzahl der Tage\n",
    "def hidden_dims(days:int, df:pd.DataFrame):\n",
    "    try:\n",
    "        df['days_past']\n",
    "        df['article_drive_id']\n",
    "        df['views_for_certain_day']\n",
    "    except:\n",
    "        print(\"The given Dataset has no column named 'days_past', 'article_drive_id' or 'views_for_certain_day'\")\n",
    "        raise\n",
    "    return df.loc[df.days_past < days]\n",
    "\n",
    "# take the average and normalize the past days based on that average\n",
    "def normalizer(normalize_value:int, past_days:list):\n",
    "    # starting with the first day by 1.0\n",
    "    normalized_list = [(normalize_value/normalize_value)]\n",
    "    #normalize all values in the list based on the average\n",
    "    past_days_normalized = [x / normalize_value for x in past_days]   \n",
    "    normalized_list = normalized_list+past_days_normalized\n",
    "    days_past = list(range(0,len(normalized_list)))\n",
    "    return normalized_list, days_past\n",
    "\n",
    "# here we generate the average on the given days\n",
    "def normalize_views(days:int, average_of_days:int, df:pd.DataFrame):\n",
    "    \n",
    "    # throw exception if averagedays are bigger than days\n",
    "    if days <= average_of_days:\n",
    "        raise Exception('the given values are invalid')\n",
    "        \n",
    "    # get the relevant days we need to consider\n",
    "    relevant_days = days+average_of_days\n",
    "    \n",
    "    # get the relevant rows and just work with that\n",
    "    df = hidden_dims((relevant_days), df)\n",
    "    df_av = pd.DataFrame({'article_drive_id':[], \n",
    "                          'views_for_certain_day_normalized':[],\n",
    "                          'meta.publisher':[]})\n",
    "    days_past_list = []\n",
    "    unique_drive_id = list(set(df['article_drive_id']))\n",
    "    for i in range(len(unique_drive_id)):\n",
    "        #get all the information of the i.- article\n",
    "        i_article = df.loc[df.article_drive_id == unique_drive_id[i]]\n",
    "\n",
    "        #filter all irrelevant days out\n",
    "        i_article = i_article.loc[i_article.days_past < (average_of_days+days)]\n",
    "        \n",
    "        #calculate the views of all given days in average_days\n",
    "        average_views = i_article.loc[i_article.days_past < average_of_days].views_for_certain_day.mean()\n",
    "        #get all days after the average calculated days\n",
    "        past_average_views = i_article.loc[i_article.days_past >= average_of_days].views_for_certain_day\n",
    "        \n",
    "        #normalize the values\n",
    "        normalized_list, days_past = normalizer(average_views, list(past_average_views))\n",
    "        days_past_list.extend(days_past)\n",
    "        \n",
    "        #get publisher of corresponding article\n",
    "        publisher = list(df.loc[df['article_drive_id'] == unique_drive_id[i]]['meta.publisher'])[0]\n",
    "        df_av = df_av.append({'views_for_certain_day_normalized': normalized_list,\n",
    "                             'article_drive_id': unique_drive_id[i],\n",
    "                             'meta.publisher': publisher}, ignore_index=True)\n",
    "        # unnest lists\n",
    "        df_av = df_av.explode(column='views_for_certain_day_normalized')\n",
    "    \n",
    "    #days past from new range we are considering\n",
    "    df_av['days_past'] = days_past_list\n",
    "    return df_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first 79 days\n",
    "days_count_norm = 80\n",
    "\n",
    "df_non_ev_norm = normalize_views(days_count_norm, 5, df_non_ev)\n",
    "df_event_ev_norm = normalize_views(days_count_norm, 5, df_event_ev)\n",
    "df_zeitlos_ev_norm = normalize_views(days_count_norm, 5, df_zeitlos_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving clicks of each evergreentype to a list\n",
    "non_ev_clicks = []\n",
    "event_ev_clicks = []\n",
    "zeitlos_ev_clicks = []\n",
    "# for iterating thru 181 days\n",
    "day_count = set(df['days_past'])\n",
    "# take median to avoid outliners\n",
    "for i in range(days_count_norm):\n",
    "    non_ev_total = df_non_ev_norm.loc[df_non_ev_norm['days_past'] == i]['views_for_certain_day_normalized']\n",
    "    non_ev_clicks.append(non_ev_total.median())\n",
    "    \n",
    "    event_ev_total = df_event_ev_norm.loc[df_event_ev_norm['days_past'] == i]['views_for_certain_day_normalized']\n",
    "    event_ev_clicks.append(event_ev_total.median())\n",
    "    \n",
    "    zeitlos_ev_total = df_zeitlos_ev_norm.loc[df_zeitlos_ev_norm['days_past'] == i]['views_for_certain_day_normalized']\n",
    "    zeitlos_ev_clicks.append(zeitlos_ev_total.median())\n",
    "\n",
    "# becaus event_ev_article nearly 0 in the future\n",
    "event_ev_clicks = [0 if x != x else x for x in event_ev_clicks]\n",
    "\n",
    "#plotting results\n",
    "evergreen_click_types = pd.DataFrame(list(zip(non_ev_clicks, event_ev_clicks, zeitlos_ev_clicks)), \n",
    "                  columns =['Non-Evergreen', 'Event-Evergreen', 'Zeitlos-Evergreen'])\n",
    "\n",
    "fig = px.line(evergreen_click_types)\n",
    "fig.update_layout(title='Average clicks of each evergreen type based on the 80 Days (with normalization)',\n",
    "                   xaxis_title='Days',\n",
    "                   yaxis_title='Clicks')\n",
    "#fig.show()\n",
    "\n",
    "fig.write_image(\"fig1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pageviews\n",
    "It is noticeable that both nonevergreen as well as evergreen articles have significant popularity differences between the first four days and the days after. However, the decrease of nonevergreen pageviews are way heavier than Evergreen articles. Thus, we can consider Evergreen articles as more consistent when it comes to pageviews over time.\n",
    "\n",
    "## Convert into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    return (1 - spatial.distance.cosine(vector1, vector2))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between:\n",
      "event_ev and non_ev_curve 55.87117952118589 %\n",
      "zeitlos_ev and non_ev 73.00445690960117 %\n",
      "event_ev and non_ev 91.1965602662992 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# convert to vectors\n",
    "non_ev_curve = np.array(non_ev_clicks)\n",
    "event_ev_curve = np.array(event_ev_clicks)\n",
    "zeitlos_ev_curve = np.array(zeitlos_ev_clicks)\n",
    "\n",
    "# now lets check if our assumption is correct. \n",
    "print('Similarity between:\\nevent_ev and non_ev_curve', cosine_similarity(event_ev_curve, non_ev_curve), '%')\n",
    "print('zeitlos_ev and non_ev', cosine_similarity(zeitlos_ev_curve, non_ev_curve), '%')\n",
    "print('event_ev and non_ev', cosine_similarity(zeitlos_ev_curve, event_ev_curve), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fullbatch we can observe that event_evs and zeitlos_evs are pretty similar, as expected. ~91% similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all to one df\n",
    "df_norm = df_non_ev_norm\n",
    "df_norm = df_norm.append(df_event_ev_norm, ignore_index=True)\n",
    "df_norm = df_norm.append(df_zeitlos_ev_norm, ignore_index=True)\n",
    "\n",
    "#drop nans\n",
    "df_norm = df_norm[~df_norm.views_for_certain_day_normalized.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are some article which have not gained views until 80 days. Thus we pad them with their median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-130e1208e8a9>:14: DeprecationWarning:\n",
      "\n",
      "The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "article_drive_id = []\n",
    "vector_length = []\n",
    "\n",
    "# get all views as vectors\n",
    "for i in set(df_norm.article_drive_id):\n",
    "    # get the vector as list\n",
    "    list_vector = df_norm.loc[df_norm.article_drive_id == i].views_for_certain_day_normalized\n",
    "    list_length = int(df_norm.loc[df_norm.article_drive_id == i].days_past[-1:])\n",
    "    \n",
    "    #pad with median if vector is not 81\n",
    "    if list_length < 80:\n",
    "        pad_len = 79-list_length\n",
    "        list_vector = list_vector.append(pd.Series(pad_len*[list_vector.median()]))\n",
    "    \n",
    "    # if vector is too big\n",
    "    else: \n",
    "        list_vector=list_vector[:80]\n",
    "    \n",
    "    article_drive_id.append(i)\n",
    "    vectors.append(np.array(np.float64(list_vector)))\n",
    "    vector_length.append(list_length)\n",
    "\n",
    "df_vector = pd.DataFrame({'article_drive_id': article_drive_id,'vector_views':vectors,\n",
    "                         'vector_length': vector_length})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels\n",
    "df_vector = df_vector.join(df.set_index('article_drive_id')[['label_id', 'label_text']], on='article_drive_id')\n",
    "df_vector.drop_duplicates(subset=['article_drive_id'],inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for just one article\n",
    "# similarity between nonevergreen article and nonev curve and evergreencurve\n",
    "def curve_similarity(vector):\n",
    "    non_sim = cosine_similarity(vector,non_ev_curve)\n",
    "    zt_sim = cosine_similarity(vector,zeitlos_ev_curve)\n",
    "    event_sim = cosine_similarity(vector,event_ev_curve)\n",
    "    return non_sim, zt_sim, event_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = df_vector.vector_views.map(curve_similarity)\n",
    "\n",
    "#initialize similarities on df\n",
    "df_vector['NonEv_similarity'] = [i[0] for i in liste]\n",
    "df_vector['ZtEv_similarity'] =  [i[1] for i in liste]\n",
    "df_vector['EventEv_similarity'] = [i[2] for i in liste]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_drive_id</th>\n",
       "      <th>vector_views</th>\n",
       "      <th>vector_length</th>\n",
       "      <th>label_id</th>\n",
       "      <th>label_text</th>\n",
       "      <th>NonEv_similarity</th>\n",
       "      <th>ZtEv_similarity</th>\n",
       "      <th>EventEv_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaa6c5d863e084b13d62b69e68631d37</td>\n",
       "      <td>[1.0, 0.3275862068965517, 0.4827586206896552, ...</td>\n",
       "      <td>79</td>\n",
       "      <td>175</td>\n",
       "      <td>kein Evergreen</td>\n",
       "      <td>65.538144</td>\n",
       "      <td>76.149688</td>\n",
       "      <td>68.533630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>da16807c9563b97c952b4cec67e30f14</td>\n",
       "      <td>[1.0, 0.17897091722595077, 0.07829977628635346...</td>\n",
       "      <td>34</td>\n",
       "      <td>175</td>\n",
       "      <td>kein Evergreen</td>\n",
       "      <td>61.953485</td>\n",
       "      <td>81.248978</td>\n",
       "      <td>82.691215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59a82ec9baa0dacdb8909c3af7fa58f2</td>\n",
       "      <td>[1.0, 0.29296875, 0.166015625, 0.17578125, 0.2...</td>\n",
       "      <td>48</td>\n",
       "      <td>175</td>\n",
       "      <td>kein Evergreen</td>\n",
       "      <td>90.440070</td>\n",
       "      <td>87.100372</td>\n",
       "      <td>76.974722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20ffaeb7522b768f0af55874fdd7133e</td>\n",
       "      <td>[1.0, 0.19704433497536947, 0.15599343185550082...</td>\n",
       "      <td>79</td>\n",
       "      <td>175</td>\n",
       "      <td>kein Evergreen</td>\n",
       "      <td>64.805244</td>\n",
       "      <td>85.828763</td>\n",
       "      <td>80.049386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8b69803346537c63ba4a3ed037657323</td>\n",
       "      <td>[1.0, 0.0693481276005548, 0.3311373092926491, ...</td>\n",
       "      <td>35</td>\n",
       "      <td>175</td>\n",
       "      <td>kein Evergreen</td>\n",
       "      <td>90.982376</td>\n",
       "      <td>60.439024</td>\n",
       "      <td>43.463740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>f662eeb33c8a940fd32d1169a62a690f</td>\n",
       "      <td>[1.0, 0.08645044637812183, 0.0966210871284891,...</td>\n",
       "      <td>80</td>\n",
       "      <td>175</td>\n",
       "      <td>kein Evergreen</td>\n",
       "      <td>98.664679</td>\n",
       "      <td>65.467078</td>\n",
       "      <td>47.964706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>aa435477e3d0ebbbed66251530602219</td>\n",
       "      <td>[1.0, 0.056782985752632666, 0.0794961800536857...</td>\n",
       "      <td>71</td>\n",
       "      <td>174</td>\n",
       "      <td>Evergreen: eventbasiert</td>\n",
       "      <td>98.443464</td>\n",
       "      <td>64.029194</td>\n",
       "      <td>45.910247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>8ad44fb5f98f8eab6c1d92d0133abbf3</td>\n",
       "      <td>[1.0, 0.04157757186980281, 0.06533618436683299...</td>\n",
       "      <td>51</td>\n",
       "      <td>175</td>\n",
       "      <td>kein Evergreen</td>\n",
       "      <td>97.624163</td>\n",
       "      <td>64.078232</td>\n",
       "      <td>46.773123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>c95b7d8b90d947d7748e563e6c06ff7e</td>\n",
       "      <td>[1.0, 0.04555010511562719, 0.03309195670793428...</td>\n",
       "      <td>65</td>\n",
       "      <td>175</td>\n",
       "      <td>kein Evergreen</td>\n",
       "      <td>86.732323</td>\n",
       "      <td>66.729685</td>\n",
       "      <td>56.561029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>d7b9809ff7011e01d433d708db09c219</td>\n",
       "      <td>[1.0, 0.1282654742292304, 0.05967790740678479,...</td>\n",
       "      <td>80</td>\n",
       "      <td>175</td>\n",
       "      <td>kein Evergreen</td>\n",
       "      <td>98.076803</td>\n",
       "      <td>62.721735</td>\n",
       "      <td>44.240120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     article_drive_id  \\\n",
       "0    aaa6c5d863e084b13d62b69e68631d37   \n",
       "1    da16807c9563b97c952b4cec67e30f14   \n",
       "2    59a82ec9baa0dacdb8909c3af7fa58f2   \n",
       "3    20ffaeb7522b768f0af55874fdd7133e   \n",
       "4    8b69803346537c63ba4a3ed037657323   \n",
       "..                                ...   \n",
       "306  f662eeb33c8a940fd32d1169a62a690f   \n",
       "307  aa435477e3d0ebbbed66251530602219   \n",
       "308  8ad44fb5f98f8eab6c1d92d0133abbf3   \n",
       "309  c95b7d8b90d947d7748e563e6c06ff7e   \n",
       "310  d7b9809ff7011e01d433d708db09c219   \n",
       "\n",
       "                                          vector_views  vector_length  \\\n",
       "0    [1.0, 0.3275862068965517, 0.4827586206896552, ...             79   \n",
       "1    [1.0, 0.17897091722595077, 0.07829977628635346...             34   \n",
       "2    [1.0, 0.29296875, 0.166015625, 0.17578125, 0.2...             48   \n",
       "3    [1.0, 0.19704433497536947, 0.15599343185550082...             79   \n",
       "4    [1.0, 0.0693481276005548, 0.3311373092926491, ...             35   \n",
       "..                                                 ...            ...   \n",
       "306  [1.0, 0.08645044637812183, 0.0966210871284891,...             80   \n",
       "307  [1.0, 0.056782985752632666, 0.0794961800536857...             71   \n",
       "308  [1.0, 0.04157757186980281, 0.06533618436683299...             51   \n",
       "309  [1.0, 0.04555010511562719, 0.03309195670793428...             65   \n",
       "310  [1.0, 0.1282654742292304, 0.05967790740678479,...             80   \n",
       "\n",
       "     label_id               label_text  NonEv_similarity  ZtEv_similarity  \\\n",
       "0         175           kein Evergreen         65.538144        76.149688   \n",
       "1         175           kein Evergreen         61.953485        81.248978   \n",
       "2         175           kein Evergreen         90.440070        87.100372   \n",
       "3         175           kein Evergreen         64.805244        85.828763   \n",
       "4         175           kein Evergreen         90.982376        60.439024   \n",
       "..        ...                      ...               ...              ...   \n",
       "306       175           kein Evergreen         98.664679        65.467078   \n",
       "307       174  Evergreen: eventbasiert         98.443464        64.029194   \n",
       "308       175           kein Evergreen         97.624163        64.078232   \n",
       "309       175           kein Evergreen         86.732323        66.729685   \n",
       "310       175           kein Evergreen         98.076803        62.721735   \n",
       "\n",
       "     EventEv_similarity  \n",
       "0             68.533630  \n",
       "1             82.691215  \n",
       "2             76.974722  \n",
       "3             80.049386  \n",
       "4             43.463740  \n",
       "..                  ...  \n",
       "306           47.964706  \n",
       "307           45.910247  \n",
       "308           46.773123  \n",
       "309           56.561029  \n",
       "310           44.240120  \n",
       "\n",
       "[311 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only work with data which has atleast views until day 66\n",
    "#df_vector = df_vector.loc[df_vector.vector_length > 65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get me all smilarities for nonevs\n",
    "zt_fails = df_vector.loc[df_vector.label_id==172]\n",
    "# get all fails which are too similar to nonev (90% or more)\n",
    "zt_fails = zt_fails.loc[zt_fails.NonEv_similarity >90].vector_views\n",
    "zt_fails.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ev_fails = df_vector.loc[df_vector.label_id==174]\n",
    "ev_fails = ev_fails.loc[ev_fails.NonEv_similarity >90].vector_views\n",
    "ev_fails.reset_index(drop=True, inplace=True)\n",
    "\n",
    "example_curves = pd.DataFrame(list(zip(zt_fails[1], zt_fails[2], zt_fails[3],ev_fails[0], ev_fails[1])))\n",
    "\n",
    "\n",
    "fig = px.line(example_curves)\n",
    "fig.update_layout(title='Evergreens behaving like non-evergreens',\n",
    "                   xaxis_title='Days',\n",
    "                   yaxis_title='Clicks')\n",
    "#fig.show()\n",
    "\n",
    "fig.write_image(\"fig2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig2.png">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_fails = df_vector.loc[df_vector.label_id==175]\n",
    "non_fails = non_fails.loc[non_fails.ZtEv_similarity >90].vector_views\n",
    "non_fails.reset_index(drop=True, inplace=True)\n",
    "\n",
    "example_curves = pd.DataFrame(list(zip(non_fails[0], non_fails[1], non_fails[2])))\n",
    "\n",
    "\n",
    "fig = px.line(example_curves)\n",
    "fig.update_layout(title='NonEvergreens behaving like Evergreens',\n",
    "                   xaxis_title='Days',\n",
    "                   yaxis_title='Clicks')\n",
    "#fig.show()\n",
    "\n",
    "fig.write_image(\"fig3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig3](https://github.com/OweysMomenzada/Evergreen-Content-Classifier-for-german-Text/blob/main/EDA/images/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "This approach would have been a great classifier since it has the most reliable performance and indeed the best accuracy.\n",
    "\n",
    "However there are still some issues to be considered if we want to use it as a data extension:\n",
    "- To get quality data, we need atleast 80 days of views\n",
    "- We need to define an high threshhold. For instance deciding wether a article is evergreen or not, everything under 90% similarity to evergreens and more than 80% similarity to NonEvs are too unreliable. Therefore we need a huge pile of data so we can get valuable data after filtering it based on that approach. And we still can not guarantee to get enough data\n",
    "- There will still be nonevergreens behaving like evergreens and vice versa. We could take a risk of wrong labeling or just let it check by an human if the labels are correct.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
